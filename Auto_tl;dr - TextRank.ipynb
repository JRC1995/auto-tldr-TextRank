{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Summarization based on sentence ranking using TextRank\n",
    "\n",
    "This is a simple implementation of extractive summarization based on sentence ranking using [TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "\n",
    "Here's a sample summarization executed by this implementation:\n",
    "\n",
    "### ORIGINAL TEXT: \n",
    "[(Source)](https://www.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/)\n",
    "\n",
    "Autotldr is a bot that uses SMMRY to create a TL;DR/summary. I will put forth points that address the effects this bot has on the reddit community.\n",
    "\n",
    "It doesn't create laziness, it only responds to it\n",
    "\n",
    "For the users who click the article link first and then return back to the comments, they will have already given their best attempt of fully reading the article. If they read it fully, the tl;dr is unneeded and ignored. If they skimmed or skipped it, the bot will be useful to at least provide more context to the discussion, like an extension of the title. A large portion of users, especially in the defaulted mainstream subreddits like /r/politics, don't even go to the article and go straight to the comments section. Most of the time, if I skip to the comments, I'm able to illicit some sort of understanding of what the article was about from the title and discussion. However this bot is able to further improve my conjectured understanding. It did not make me skip it, it only helped me when I already decided to skip it. The scenario in which this bot would create a significantly lazy atmosphere is if the tl;dr were to be presented parallel to the main submission, in the same way the OP's tl;dr is presented right next to the long body of self post. Also, the tl;dr becomes more prevalent/hidden as it will get upvoted/downvoted depending on how much of a demand there was for a tl;dr in the first place. If it becomes the top voted comment than it has become more of a competitor to the original text for those who go to the comments first, but by then the thread has decided that a tl;dr was useful and the bot delivered.\n",
    "\n",
    "It can make sophisticated topics more relevant to mainstream Reddit\n",
    "\n",
    "Sophisticated and important topics are usually accompanied or presented by long detailed articles. By making these articles and topics relevant to a larger portion of the Reddit userbase (those who weren't willing to read the full article), it popularizes the topic and increases user participation. These posts will get more attention in the form of upvotes/downvotes, comments, and reposts. This will increase the prevalence of sophisticated topics in the mainstream subreddits and compete against cliched memes. This has the potential of re-sophisticating the topic discussion in the mainstream subreddits, as more hardcore redditors don't have to retreat to a safe haven like /r/TrueReddit. This is a loose approximation and the magnitude of this effect is questionable, but I'm not surprised if the general direction of the theory is correct. I'm not claiming this would improve reddit overnight, but instead very very gradually.\n",
    "\n",
    "It decreases Reddit's dependency on external sites\n",
    "\n",
    "The bot doubles as a context provider for when a submission link goes down, is removed, or inaccessible at work/school. The next time the article you clicked gives you a 404 error, you won't have to depend on the users to provide context as the bot will have been able to provide that service at a much faster and consistent rate than a person. Additionally, an extended summary is posted in /r/autotldr, which acts as a perpetual archive and decreases how much reddit gets broken by external sites.\n",
    "\n",
    "Only useful tl;dr's are posted\n",
    "\n",
    "There are several criteria for a bot to post a tl;dr. It posts the three most important sentences as decided by the core algorithm, and they must be within 450-700 characters total. The final tl;dr must also be 70% smaller than the original, that way there is a big gap between the original and the tl;dr, hence only very long articles get posted on. This way the likelihood of someone nonchalantly declaring \"TL;DR\" in a thread and the bot posting in the same one is high. Also my strategy is to tell the bot to post in default, mainstream subreddits were the demand for a TL;DR is much higher than /r/TrueReddit and /r/worldevents.\n",
    "\n",
    "Feel free to respond to these concepts and to raise your own. Be polite, respectful, and clarify what you say. Any offending posts to this rule will be removed.\n",
    "\n",
    "\n",
    "### GENERATED SUMMARY:\n",
    "\n",
    "Autotldr is a bot that uses SMMRY to create a TL ; DR/summary.\n",
    "\n",
    "The scenario in which this bot would create a significantly lazy atmosphere is if the tl ; dr were to be presented parallel to the main submission , in the same way the OP 's tl ; dr is presented right next to the long body of self post.\n",
    "\n",
    "If it becomes the top voted comment than it has become more of a competitor to the original text for those who go to the comments first , but by then the thread has decided that a tl ; dr was useful and the bot delivered.\n",
    "\n",
    "By making these articles and topics relevant to a larger portion of the Reddit userbase ( those who were n't willing to read the full article ) , it popularizes the topic and increases user participation.\n",
    "\n",
    "Only useful tl ; dr 's are posted There are several criteria for a bot to post a tl ; dr.\n",
    "\n",
    "The final tl ; dr must also be 70 % smaller than the original , that way there is a big gap between the original and the tl ; dr , hence only very long articles get posted on.\n",
    "\n",
    "\n",
    "### Here starts the code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'summarytest.txt' #Enter Filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data into Python variable from the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filename,'r')\n",
    "Text = \"\"\n",
    "for line in file.readlines():\n",
    "    Text+=str(line)\n",
    "    Text+=\" \"\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-printable characters from data and tokenizing the resultant test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "def clean(text):\n",
    "    printable = set(string.printable)\n",
    "    text = filter(lambda x: x in printable, text) #filter funny characters, if any.\n",
    "    return text\n",
    "\n",
    "Cleaned_text = clean(Text)\n",
    "\n",
    "text = word_tokenize(Cleaned_text)\n",
    "case_insensitive_text = word_tokenize(Cleaned_text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation\n",
    "\n",
    "Senteces is a list of segmented sentences in the natural form with case sensitivity. This will be\n",
    "necessary for displaying the summary later on.\n",
    "\n",
    "Tokenized_sentences is a list of tokenized segmented sentences without case sensitivity. This will be\n",
    "useful for text processing later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Segmentation\n",
    "\n",
    "sentences = []\n",
    "tokenized_sentences = []\n",
    "sentence = \" \"\n",
    "for word in text:\n",
    "    if word != '.':\n",
    "        sentence+=str(word)+\" \"\n",
    "    else:\n",
    "        sentences.append(sentence.strip())\n",
    "        tokenized_sentences.append(word_tokenize(sentence.lower().strip()))\n",
    "        sentence = \" \"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization of tokenized words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(POS_tagged_text):\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    adjective_tags = ['JJ','JJR','JJS']\n",
    "    lemmatized_text = []\n",
    "    \n",
    "    for word in POS_tagged_text:\n",
    "        if word[1] in adjective_tags:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "        else:\n",
    "            lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "#Pre_processing:\n",
    "\n",
    "POS_tagged_text = nltk.pos_tag(case_insensitive_text)\n",
    "lemmatized_text = lemmatize(POS_tagged_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging lemmatized words\n",
    "\n",
    "This will be useful for generating stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_text = nltk.pos_tag(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Generation\n",
    "\n",
    "Based on the assumption that typically only nouns and adjectives are qualified as parts of keyword phrases, I will include any word that aren't tagged as a noun or adjective to the list of stopwords. (Note: Gerunds can often be important keywords or components of it. But including words tagged as 'VBG' (tag for present participles and gerunds) also include verbs of present continiuous forms which should be treated as stopwords. So I am not adding 'VBG' to list of POS that should not be treated as 'stopword-POSs'. Punctuations will be added to the same list (of stopwords). Additional the long list of stopwords from https://www.ranks.nl/stopwords are also added to the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stopwords(POS_tagged_text):\n",
    "    stopwords = []\n",
    "    \n",
    "    wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','FW'] #may be add VBG too\n",
    "    \n",
    "    for word in POS_tagged_text:\n",
    "        if word[1] not in wanted_POS:\n",
    "            stopwords.append(word[0])\n",
    "            \n",
    "    punctuations = list(str(string.punctuation))\n",
    "    stopwords = stopwords + punctuations\n",
    "    \n",
    "    stopword_file = open(\"long_stopwords.txt\", \"r\")\n",
    "    #Source = https://www.ranks.nl/stopwords\n",
    "\n",
    "    for line in stopword_file.readlines():\n",
    "        stopwords.append(str(line.strip()))\n",
    "\n",
    "    return set(stopwords)\n",
    "\n",
    "stopwords = generate_stopwords(Processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing tokenized sentences.\n",
    "\n",
    "Lemmatizing words in the sentences, and removing stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    processed_sentence = []\n",
    "    \n",
    "    POS_tagged_sentence = nltk.pos_tag(sentence)\n",
    "    lemmatized_sentence = lemmatize(POS_tagged_sentence)\n",
    "\n",
    "    for word in lemmatized_sentence:\n",
    "        if word not in stopwords:\n",
    "            processed_sentence.append(word)\n",
    "    processed_sentences.append(processed_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Graph\n",
    "\n",
    "TextRank is a graph based model, and thus it requires us to build a graph. Each sentence in the text will serve as a vertex for graph. The sentences will be represented in the vertices by their index in the list of sentences\\processed_sentences. \n",
    "\n",
    "The weighted_edge matrix contains the information of edge connections among all the vertices. I am building a wieghted undirected edges.\n",
    "\n",
    "weighted_edge[i][j] contains the weight of the connecting edge between the sentence vertex represented by sentence of index i and the sentence vertex represented by sentence of index j.\n",
    "\n",
    "If weighted_edge[i][j] is zero, it means no edge connection is present between the words represented by index i and j.\n",
    "\n",
    "There is a connection between the sentences (and thus between i and j which represents them) if the the value of the similarity between the two sentences is non-zero. The weight of the connection is the value of the similary between the connected vertices.\n",
    "\n",
    "The value of the weighted_edge[i][j] is the determined by the similarity function. \n",
    "\n",
    "The similarity function is: \n",
    "(No. of overlapping words in sentence i and j)/(log(len(sentence_i)) + log(len(sentence_j))\n",
    "\n",
    "The score of all vertices are intialized to one.\n",
    "Self-connections are not considered, so weighted_edge[i][i] will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from __future__ import division\n",
    "\n",
    "sentence_len = len(processed_sentences)\n",
    "weighted_edge = np.zeros((sentence_len,sentence_len),dtype=np.float32)\n",
    "\n",
    "score = np.zeros((sentence_len),dtype=np.float32)\n",
    "\n",
    "for i in xrange(0,sentence_len):\n",
    "    score[i]=1\n",
    "    for j in xrange(0,sentence_len):\n",
    "        if j==i:\n",
    "            weighted_edge[i][j]=0\n",
    "        else:\n",
    "            for word in processed_sentences[i]:\n",
    "                if word in processed_sentences[j]:\n",
    "                    weighted_edge[i][j] += processed_sentences[j].count(word)\n",
    "            if weighted_edge[i][j]!=0:\n",
    "                len_i = len(processed_sentences[i])\n",
    "                len_j = len(processed_sentences[j])\n",
    "                weighted_edge[i][j] = weighted_edge[i][j]/(math.log(len_i)+math.log(len_j))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating weighted summation of connections of a vertex\n",
    "\n",
    "inout[i] will contain the sum of all the undirected connections\\edges associated withe the vertex represented by i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inout = np.zeros((sentence_len),dtype=np.float32)\n",
    "\n",
    "for i in xrange(0,sentence_len):\n",
    "    for j in xrange(0,sentence_len):\n",
    "        inout[i]+=weighted_edge[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Vertices\n",
    "\n",
    "The formula used for scoring a vertex represented by i is:\n",
    "\n",
    "score[i] = (1-d) + d x [ Summation(j) ( (weighted_edge[i][j]/inout[j]) x score[j] ) ] where j belongs to the list of vertieces that has a connection with i. \n",
    "\n",
    "d is the damping factor.\n",
    "\n",
    "The score is iteratively updated until convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging at iteration 25....\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d=0.85\n",
    "threshold = 0.0001 #convergence threshold\n",
    "\n",
    "for iter in xrange(0,MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in xrange(0,sentence_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in xrange(0,sentence_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "                \n",
    "        score[i] = (1-d) + d*(summation)\n",
    "    \n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "        print \"Converging at iteration \"+str(iter)+\"....\"\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying each sentence and its corresponding score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\n",
      "\n",
      "Autotldr is a bot that uses SMMRY to create a TL ; DR/summary\n",
      "Score: 1.18796\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "I will put forth points that address the effects this bot has on the reddit community\n",
      "Score: 0.940897\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "It does n't create laziness , it only responds to it For the users who click the article link first and then return back to the comments , they will have already given their best attempt of fully reading the article\n",
      "Score: 1.05546\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "If they read it fully , the tl ; dr is unneeded and ignored\n",
      "Score: 0.828447\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "If they skimmed or skipped it , the bot will be useful to at least provide more context to the discussion , like an extension of the title\n",
      "Score: 0.834126\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "A large portion of users , especially in the defaulted mainstream subreddits like /r/politics , do n't even go to the article and go straight to the comments section\n",
      "Score: 0.990735\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "Most of the time , if I skip to the comments , I 'm able to illicit some sort of understanding of what the article was about from the title and discussion\n",
      "Score: 0.919072\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "However this bot is able to further improve my conjectured understanding\n",
      "Score: 0.727377\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "It did not make me skip it , it only helped me when I already decided to skip it\n",
      "Score: 0.15\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "The scenario in which this bot would create a significantly lazy atmosphere is if the tl ; dr were to be presented parallel to the main submission , in the same way the OP 's tl ; dr is presented right next to the long body of self post\n",
      "Score: 1.34784\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "Also , the tl ; dr becomes more prevalent/hidden as it will get upvoted/downvoted depending on how much of a demand there was for a tl ; dr in the first place\n",
      "Score: 1.13423\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "If it becomes the top voted comment than it has become more of a competitor to the original text for those who go to the comments first , but by then the thread has decided that a tl ; dr was useful and the bot delivered\n",
      "Score: 1.44883\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "It can make sophisticated topics more relevant to mainstream Reddit Sophisticated and important topics are usually accompanied or presented by long detailed articles\n",
      "Score: 1.19482\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "By making these articles and topics relevant to a larger portion of the Reddit userbase ( those who were n't willing to read the full article ) , it popularizes the topic and increases user participation\n",
      "Score: 1.43953\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "These posts will get more attention in the form of upvotes/downvotes , comments , and reposts\n",
      "Score: 0.367749\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "This will increase the prevalence of sophisticated topics in the mainstream subreddits and compete against cliched memes\n",
      "Score: 0.588198\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "This has the potential of re-sophisticating the topic discussion in the mainstream subreddits , as more hardcore redditors do n't have to retreat to a safe haven like /r/TrueReddit\n",
      "Score: 0.595447\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "This is a loose approximation and the magnitude of this effect is questionable , but I 'm not surprised if the general direction of the theory is correct\n",
      "Score: 0.15\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "I 'm not claiming this would improve reddit overnight , but instead very very gradually\n",
      "Score: 0.484023\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "It decreases Reddit 's dependency on external sites The bot doubles as a context provider for when a submission link goes down , is removed , or inaccessible at work/school\n",
      "Score: 1.00579\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "The next time the article you clicked gives you a 404 error , you wo n't have to depend on the users to provide context as the bot will have been able to provide that service at a much faster and consistent rate than a person\n",
      "Score: 1.0597\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "Additionally , an extended summary is posted in /r/autotldr , which acts as a perpetual archive and decreases how much reddit gets broken by external sites\n",
      "Score: 0.472735\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "Only useful tl ; dr 's are posted There are several criteria for a bot to post a tl ; dr\n",
      "Score: 1.64419\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "It posts the three most important sentences as decided by the core algorithm , and they must be within 450-700 characters total\n",
      "Score: 0.15\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "The final tl ; dr must also be 70 % smaller than the original , that way there is a big gap between the original and the tl ; dr , hence only very long articles get posted on\n",
      "Score: 1.34355\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "This way the likelihood of someone nonchalantly declaring `` TL ; DR '' in a thread and the bot posting in the same one is high\n",
      "Score: 1.15188\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "Also my strategy is to tell the bot to post in default , mainstream subreddits were the demand for a TL ; DR is much higher than /r/TrueReddit and /r/worldevents\n",
      "Score: 1.23764\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "Feel free to respond to these concepts and to raise your own\n",
      "Score: 0.15\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "Be polite , respectful , and clarify what you say\n",
      "Score: 0.15\n",
      "\n",
      "\n",
      "Sentence:\n",
      "\n",
      "Any offending posts to this rule will be removed\n",
      "Score: 0.15\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for sentence in sentences:\n",
    "    print \"Sentence:\\n\\n\"+str(sentence)+\"\\nScore: \"+str(score[i])+\"\\n\\n\"\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Generation\n",
    "\n",
    "Given some hyperparameters the program computes the summary_size. Sentences are then ranked in accordance to their corresponding scores. \n",
    "\n",
    "More precisely, the indices of the sentences are sorted based on the scores of their corresponding sentences. Based on size of the summary, indices of top 'summary_size' no. of highest scoring input sentences are chosen for generating the summary. \n",
    "\n",
    "The summary is then generated by presenting the sentences (whose indices were chosen) in a chronological order.\n",
    "\n",
    "Note: I hardcoded the selection of the first statement (if the summary_size is computed to be more than 1) because the first sentence can usually serve as an introduction, and provide some context to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUMMARY: \n",
      "\n",
      "Autotldr is a bot that uses SMMRY to create a TL ; DR/summary.\n",
      "\n",
      "The scenario in which this bot would create a significantly lazy atmosphere is if the tl ; dr were to be presented parallel to the main submission , in the same way the OP 's tl ; dr is presented right next to the long body of self post.\n",
      "\n",
      "If it becomes the top voted comment than it has become more of a competitor to the original text for those who go to the comments first , but by then the thread has decided that a tl ; dr was useful and the bot delivered.\n",
      "\n",
      "By making these articles and topics relevant to a larger portion of the Reddit userbase ( those who were n't willing to read the full article ) , it popularizes the topic and increases user participation.\n",
      "\n",
      "Only useful tl ; dr 's are posted There are several criteria for a bot to post a tl ; dr.\n",
      "\n",
      "The final tl ; dr must also be 70 % smaller than the original , that way there is a big gap between the original and the tl ; dr , hence only very long articles get posted on.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Reduce_to_percent = 20\n",
    "summary_size = int(((Reduce_to_percent)/100)*len(sentences))\n",
    "\n",
    "if summary_size == 0:\n",
    "    summary_size = 1\n",
    "\n",
    "sorted_sentence_score_indices = np.flip(np.argsort(score),0)\n",
    "\n",
    "indices_for_summary_results = sorted_sentence_score_indices[0:summary_size]\n",
    "\n",
    "summary = \"\\n\"\n",
    "\n",
    "current_size = 0\n",
    "\n",
    "if 0 not in indices_for_summary_results and summary_size!=1:\n",
    "    summary+=sentences[0]\n",
    "    summary+=\".\\n\\n\"\n",
    "    current_size+=1\n",
    "\n",
    "\n",
    "for i in xrange(0,len(sentences)):\n",
    "    if i in indices_for_summary_results:\n",
    "        summary+=sentences[i]\n",
    "        summary+=\".\\n\\n\"\n",
    "        current_size += 1\n",
    "    if current_size == summary_size:\n",
    "        break\n",
    "\n",
    "print \"\\nSUMMARY: \"\n",
    "print summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
